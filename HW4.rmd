```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.width = 6)
knitr::opts_chunk$set(fig.align="center")
```


## Loading relevant libraries 

```{r message=F}
library(dplyr)
library(tidyverse)
library(caret)
library(glmnet)
library(ROCR)
library(dbarts)
library(xgboost)
library(ranger)
library(rpart)
library(rpart.plot)
library(ggplot2)
```

# Question 2
## Reading the data
```{r}
housing.train = read.csv('housing_train.csv')
housing.test = read.csv('housing_test.csv')
housing.train$logSP = log(housing.train$SalePrice)
housing.test$logSP = log(housing.test$SalePrice)
```
## 2.1
### Histogram of $SalePrice$
```{r}
housing.train %>% hist(SalePrice)
housing.test %>% hist(SalePrice)
```

### Histogram of $log(SalePrice)$
```{r}
housing.train %>% hist(logSP)
housing.test %>% hist(logSP)
```
### Scatterplot between $SalePrice$ and $Gr_Liv_Area$

### Scatterplot between $log(SalePrice)$ and $Gr_Liv_Area$

## 2.2
```{r}
#' @param y: a vector of responses
#' @param X: covariate matrix
#' @return plots of RSS and BIC as a function of model size
run_forward_selection_BIC = function(y, X){
  S = store_RSS = c()
  p = dim(X)[2]
  
  for(i in 1:p){
  		RSS = rep(0,p)
  		for(j in 1:p){
  		  S0 = append(S,j)
  		  RSS[j] = sum((y-lm(y~X[,S0])$fitted.values)^2)
  		}
  		store_RSS[i+1] = min(RSS)
  		ind = which.min(RSS)
  		S = append(S,ind)
  }
  
  	BIC = n*log(store_RSS) + (0:p)*log(n)
  	modelsize = which.min(BIC)-1
  	
		par(mfrow=c(1,2))
		plot(0:p, store_RSS, xlab='Model Size', ylab='RSS')
  	plot(0:p, BIC, xlab='Model Size', ylab='BIC')
}
```

```{r}
run_forward_selection_BIC(y, X)
```

### Best Subset

Compares all models for each possible combination of the $p$ predictors. Since there are $2^p$ models to compare, it cannot be applied when $p$ is large.

```{r, message=F}
library(bestglm)

data = cbind(X,y)
bestglm(as.data.frame(data), family = gaussian, IC = "BIC", method = "exhaustive")
```
## 2.3
**Lasso**: $\hat{\beta} = \text{argmin}_{\beta} \Big\{ \text{deviance} +\lambda \|\beta\|_1\Big\}$
```{r}
ames = as.matrix(read.table('ames_data.txt', header=TRUE))

y_name = 'LotArea'

ind = which(ames[,colnames(ames)=='YearBuilt']>=2000) # only those built after 2000

X = ames[ind, colnames(ames)!=y_name]
y = ames[ind, colnames(ames)==y_name]

n = length(y); p = dim(X)[2]

colnames(X)
```

Lasso regression:
```{r fig.height=6}
plot(glmnet(X, y, alpha=1), xvar = "lambda") # alpha = 1 is lasso penalty
```

### Cross-Validation
```{r fig.height=4}
# default is 10-fold cv
cv_lasso = cv.glmnet(X, y, alpha=1) # lasso

par(mfrow=c(1,2))
plot(cv_lasso)
```

```{r}
coef(cv_lasso)
```

```{r}
predict(cv_lasso, housing.test)
```
## 2.4

## 2.5

```{r}
lm.fit = lm(log(Sale_Price) ~ . , as.data.frame(housing.train))
yhat = exp(predict(lm.fit, as.data.frame(housing.test)))
#sampleSubmission is a dataframe with columns 'Id' and 'count'
sampleSubmission = data.frame(Id=1:length(yhat), Sale_Price=yhat)
write.csv(sampleSubmission,
          file = "sampleSubmission.csv",
          row.names = FALSE,
          quote = FALSE)
```

